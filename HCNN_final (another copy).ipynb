{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchquantum as tq\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "from skimage import io\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define controlled hadamard gate\n",
    "sq2 = 1/np.sqrt(2)\n",
    "def controlled_H(qdev, target,control):\n",
    "      qdev.apply(tq.QubitUnitary(\n",
    "      has_params=True,init_params=([[1,0,0,0],[0,sq2,0,sq2],[0,0,1,0],[0,sq2,0,-sq2]]),wires=[target,control]))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `view` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 256, 256])\n",
      "tensor([[0.3621, 0.1498, 0.5706,  ..., 0.4156, 0.5716, 0.8559],\n",
      "        [0.3958, 0.8412, 0.7895,  ..., 0.1166, 0.7951, 0.3923],\n",
      "        [0.9597, 0.2227, 0.6577,  ..., 0.3778, 0.2328, 0.2865],\n",
      "        ...,\n",
      "        [0.6649, 0.6406, 0.9665,  ..., 0.0784, 0.1857, 0.0200],\n",
      "        [0.1110, 0.8320, 0.9081,  ..., 0.8613, 0.7684, 0.9758],\n",
      "        [0.3633, 0.7559, 0.1300,  ..., 0.3864, 0.7073, 0.4571]])\n"
     ]
    }
   ],
   "source": [
    "# x is the input \n",
    "x = torch.tensor([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])\n",
    "x = torch.rand(100,256,256)\n",
    "\n",
    "print(x.shape)\n",
    "#x = x.view(4,16) # to reshape the tensor\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8947, 0.7980, 0.2596,  ..., 0.5840, 0.6783, 0.1627],\n",
      "        [0.4275, 0.7236, 0.1614,  ..., 0.1128, 0.8175, 0.6031],\n",
      "        [0.7143, 0.1140, 0.6310,  ..., 0.3187, 0.3684, 0.1996],\n",
      "        ...,\n",
      "        [0.3339, 0.9225, 0.6631,  ..., 0.8142, 0.4090, 0.5994],\n",
      "        [0.0814, 0.0967, 0.2155,  ..., 0.6682, 0.4648, 0.0070],\n",
      "        [0.0378, 0.9988, 0.6430,  ..., 0.6185, 0.0587, 0.9132]])\n"
     ]
    }
   ],
   "source": [
    "print(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3621)\n",
      "tensor([0.3621, 0.0995, 0.8947, 0.6125, 0.5663, 0.2897, 0.0862, 0.2509, 0.4025,\n",
      "        0.9555, 0.8904, 0.7887, 0.4842, 0.6590, 0.9295, 0.7216, 0.2839, 0.2622,\n",
      "        0.5806, 0.3185, 0.7709, 0.1310, 0.1731, 0.1140, 0.8124, 0.4603, 0.9993,\n",
      "        0.8800, 0.4340, 0.8631, 0.0423, 0.8512, 0.3358, 0.5474, 0.7675, 0.3433,\n",
      "        0.6147, 0.7819, 0.7963, 0.8118, 0.3661, 0.4824, 0.4168, 0.9186, 0.1280,\n",
      "        0.8740, 0.9863, 0.0318, 0.3563, 0.2173, 0.6616, 0.5147, 0.8302, 0.9485,\n",
      "        0.5114, 0.9541, 0.0516, 0.5001, 0.8533, 0.7540, 0.0649, 0.6133, 0.0306,\n",
      "        0.2059, 0.0273, 0.9221, 0.0402, 0.4775, 0.1499, 0.0287, 0.3849, 0.0799,\n",
      "        0.0254, 0.7256, 0.8309, 0.5344, 0.6416, 0.4504, 0.6615, 0.1124, 0.4128,\n",
      "        0.4922, 0.3278, 0.2425, 0.8162, 0.5853, 0.1765, 0.2885, 0.1550, 0.1294,\n",
      "        0.9369, 0.5583, 0.6550, 0.5714, 0.8849, 0.5793, 0.2621, 0.6059, 0.5127,\n",
      "        0.5980])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.8412, 0.6771, 0.7236, 0.1376, 0.2603, 0.1329, 0.6727, 0.7956, 0.1888,\n",
       "        0.7975, 0.1610, 0.4217, 0.4693, 0.4494, 0.4325, 0.0736, 0.3461, 0.3917,\n",
       "        0.3693, 0.2489, 0.4091, 0.2408, 0.6728, 0.0136, 0.8628, 0.6364, 0.3646,\n",
       "        0.9044, 0.8733, 0.3287, 0.5531, 0.8773, 0.7473, 0.3041, 0.3030, 0.6875,\n",
       "        0.7789, 0.1700, 0.0968, 0.1315, 0.5975, 0.4985, 0.9761, 0.8742, 0.5603,\n",
       "        0.1389, 0.5817, 0.2464, 0.4763, 0.8450, 0.7419, 0.5835, 0.0248, 0.7725,\n",
       "        0.0601, 0.7170, 0.4996, 0.1446, 0.6374, 0.6223, 0.1454, 0.1208, 0.3583,\n",
       "        0.6223, 0.4601, 0.8802, 0.1035, 0.2576, 0.1737, 0.2086, 0.1683, 0.2175,\n",
       "        0.0831, 0.4120, 0.8211, 0.7229, 0.0578, 0.3546, 0.3049, 0.3379, 0.8907,\n",
       "        0.7936, 0.6809, 0.8609, 0.4184, 0.7515, 0.3582, 0.7728, 0.7218, 0.4607,\n",
       "        0.3423, 0.6880, 0.1857, 0.4826, 0.5414, 0.7040, 0.9894, 0.6198, 0.3273,\n",
       "        0.4306])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(x[0]) # first image\n",
    "#print(x[0,0]) # first row of the first image\n",
    "print(x[0,0,0]) # first pixel of the first row of the first image\n",
    "#print(x[0,:,0]) # first column of the first image\n",
    "print(x[:,0,0]) # list of pixels at (0,0) of all images\n",
    "x[:,0,0+1] # list of pixels at (0,1) of all images\n",
    "x[:,0+1,0] # list of pixels at (1,0) of all images\n",
    "x[:,0+1,0+1] # list of pixels at (1,1) of all images\n",
    "torch.cat((x[:, 0, 0], x[:, 0, 0+1], x[:, 0+1, 0], x[:, 0+1, 0+1])) # list of pixels at (0,0), (0,1), (1,0), (1,1) of all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x[:, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m], x[:, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m], x[:, \u001b[39m0\u001b[39m\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m], x[:, \u001b[39m0\u001b[39m\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]))\u001b[39m.\u001b[39mview(\u001b[39m4\u001b[39m, \u001b[39m100\u001b[39m) \u001b[39m# first matrix/list is the list of all pixels at (0,0). second matrix/list is the list of all pixels at (0,1). third matrix/list is the list of all pixels at (1,0). fourth matrix/list is the list of all pixels at (1,1). \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "x1 = torch.cat((x[:, 0, 0], x[:, 0, 0+1], x[:, 0+1, 0], x[:, 0+1, 0+1])).view(4, 100) # first matrix/list is the list of all pixels at (0,0). second matrix/list is the list of all pixels at (0,1). third matrix/list is the list of all pixels at (1,0). fourth matrix/list is the list of all pixels at (1,1). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtranspose(torch\u001b[39m.\u001b[39mcat((x[:, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m], x[:, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m], x[:, \u001b[39m0\u001b[39m\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m], x[:, \u001b[39m0\u001b[39m\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]))\u001b[39m.\u001b[39mview(\u001b[39m4\u001b[39m, \u001b[39m100\u001b[39m),\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m) \u001b[39m# first row contains (0,0)(0,1)(1,0)(1,1) pixels\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "x1 = torch.transpose(torch.cat((x[:, 0, 0], x[:, 0, 0+1], x[:, 0+1, 0], x[:, 0+1, 0+1])).view(4, 100),0,1) # first row contains (0,0)(0,1)(1,0)(1,1) pixels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bsz \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m# batch size\u001b[39;00m\n\u001b[1;32m      2\u001b[0m size \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m \u001b[39m# height and width of an image\u001b[39;00m\n\u001b[1;32m      3\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(bsz, size, size) \u001b[39m# reshape the data \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "bsz = x.shape[0] # batch size\n",
    "size = 256 # height and width of an image\n",
    "x = x.view(bsz, size, size) # reshape the data \n",
    "\n",
    "data_list = []\n",
    "\n",
    "for c in range(0, size, 2):\n",
    "    for r in range(0, size, 2):\n",
    "        data = torch.transpose(torch.cat((x[:, c, r], x[:, c, r+1], x[:, c+1, r], x[:, c+1, r+1])).view(4, bsz), 0, 1)\n",
    "        \n",
    "\n",
    "        data_list.append(data.view(bsz, 4))\n",
    "#\n",
    "# print(data_list) # 100 images, 4 pixels, (2 x 2 encoded in one single list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cat takes following arguments: (tuple of tensors, dim=0, out=None)\n",
    "# dim=0 means that the tensors are concatenated along the first dimension\n",
    "# torch.transpose takes following arguments: (input, dim0, dim1, out=None)\n",
    "# dim0 and dim1 are the dimensions to be swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 65536])\n",
      "torch.Size([65536])\n"
     ]
    }
   ],
   "source": [
    "result = torch.cat(data_list, dim=1).float() # concatenate all the tensors in the list along the first dimension 0th dim remains same which is the batch size.\n",
    "print(result.shape)\n",
    "print(result[0].shape) # first row is the first image concatanated in a single list with all the pixels i.e. 256 x 256 image flattened into 65536  x 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuanvolutionFilter(tq.QuantumModule):\n",
    "  # the __init__ method initializes the quantum device, the general encoder,\n",
    "  # a random quantum layer, and a measurement operator.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_wires = 4  # two ancillas\n",
    "        self.q_device = tq.QuantumDevice(n_wires=self.n_wires)\n",
    "        # encoding the input data\n",
    "        self.encoder = tq.GeneralEncoder(\n",
    "        [   {'input_idx': [0], 'func': 'ry', 'wires': [0]},\n",
    "            {'input_idx': [1], 'func': 'ry', 'wires': [1]},\n",
    "            {'input_idx': [2], 'func': 'ry', 'wires': [2]},\n",
    "            {'input_idx': [3], 'func': 'ry', 'wires': [3]},])\n",
    "        \n",
    "\n",
    "\n",
    "        # random circuit layer \n",
    "        self.q_layer = tq.RandomLayer(n_ops=8, wires=list(range(self.n_wires)))\n",
    "        self.measure = tq.MeasureAll(tq.PauliZ)\n",
    "        #self.expval = tq.expval()\n",
    "    \n",
    "# x has the dimension of (batch_size, 28, 28) representing a batch of greyscale images\n",
    "#The method first reshapes the input data into a 2D array of shape \n",
    "#(batch_size, 784) by concatenating adjacent 2x2 blocks of pixels.\n",
    "# data is the new reshaped tensor \n",
    "    def forward(self, x, use_qiskit=False):\n",
    "        bsz = x.shape[0] # batch size\n",
    "        size = 256 # height and width of an image\n",
    "        x = x.view(bsz, size, size) # reshape the data \n",
    "\n",
    "        data_list = []\n",
    "\n",
    "        for c in range(0, size, 2):\n",
    "            for r in range(0, size, 2):\n",
    "                data = torch.transpose(torch.cat((x[:, c, r], x[:, c, r+1], x[:, c+1, r], x[:, c+1, r+1])).view(4, bsz), 0, 1)\n",
    "                if use_qiskit:\n",
    "                    data = self.qiskit_processor.process_parameterized(\n",
    "                        self.q_device, self.encoder, self.q_layer, self.measure, data)\n",
    "                else:\n",
    "                    self.encoder(self.q_device, data)\n",
    "                    \n",
    "                    #haar wavelet\n",
    "                    # level 1 \n",
    "                    self.q_device.h(wires = 3) \n",
    "                    self.q_device.swap([3,2])\n",
    "                    self.q_device.swap([2,1])\n",
    "                    self.q_device.swap([1,0])\n",
    "\n",
    "                    # level 2 \n",
    "                    controlled_H(self.q_device, target=2, control= 3)\n",
    "                    self.q_device.cswap([3,2,1])\n",
    "                    self.q_device.cswap([3,1,0])\n",
    "\n",
    "                    # level 3\n",
    "                    \n",
    "                    #self.q_device.ccx([2,3,4])\n",
    "                    #controlled_H(self.q_device, target=1, control= 4)\n",
    "                    #self.q_device.ccx([2,3,4])\n",
    "                    #perm\n",
    "                    #self.q_device.ccx([2,3,4])\n",
    "                    #self.q_device.cswap([4,1,0])\n",
    "                    #self.q_device.ccx([2,3,4])\n",
    "\n",
    "                    #level 4\n",
    "                    #self.q_device.ccx([2,3,4])\n",
    "                    #self.q_device.ccx([1,4,5])\n",
    "                    #controlled_H(self.q_device, target=0, control= 5)\n",
    "                    #self.q_device.ccx([1,4,5])\n",
    "                    #self.q_device.ccx([2,3,4])\n",
    "\n",
    "\n",
    "\n",
    "                    self.q_layer(self.q_device)\n",
    "                    data = self.measure(self.q_device)\n",
    "                    \n",
    "                    #for i in range(4):\n",
    "                    #    measure_result = []\n",
    "                    #    measure_result.append(tq.expval(self.q_device,wires=i, observables= tq.PauliZ(wires=i)))\n",
    "\n",
    "                    #    data = torch.tensor([measure_result])\n",
    "\n",
    "                    \n",
    "                data_list.append(data.view(bsz, 4)) # only keep the first 4 qubits\n",
    "        \n",
    "        result = torch.cat(data_list, dim=1).float()\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HybridModel(torch.nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__() \n",
    "        self.qf = QuanvolutionFilter()\n",
    "        self.linear = torch.nn.Linear(4*128*128, 2) \n",
    "    def forward(self, x, use_qiskit=False):\n",
    "        with torch.no_grad():\n",
    "          x = self.qf(x, use_qiskit)\n",
    "        x = self.linear(x)\n",
    "        return F.softmax(x, -1) \n",
    "    # F.log_softmax is the log of the softmax function, which is a common choice for the output of a classification model.\n",
    "    \n",
    "class HybridModel_without_qf(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(256*256, 2)\n",
    "    \n",
    "    def forward(self, x, use_qiskit=False):\n",
    "        x = x.view(-1, 256*256)\n",
    "        x = self.linear(x)\n",
    "        return F.log_softmax(x, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the dataset \n",
    "class Oral_Can_Data(Dataset):\n",
    "    '''Oral cancer dataset'''\n",
    "    def __init__(self, csv_file, root_dir,transform = None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations) # returns the number of samples in the dataset\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        image = io.imread(img_path)\n",
    "        image = np.array(image)\n",
    "        image = torch.tensor([image])\n",
    "        can_type = self.annotations.iloc[index, 1] # labels:  1 for non-cancerous, 2 for cancerous\n",
    "        can_type = torch.tensor([can_type])\n",
    "        can_type = can_type\n",
    "\n",
    "    \n",
    "        sample = {'image': image, 'can_type': can_type}\n",
    "        \n",
    "        for index in range(len(self.annotations)):\n",
    "            sample\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return (sample) # returns the image and the label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "dataset = Oral_Can_Data(csv_file= '/home/iisers/Documents/oral_cancer_project/labels .csv',root_dir='/home/iisers/Documents/oral_cancer_project/Combined_data_resized')\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_set, val_set,  test_set = torch.utils.data.random_split(dataset, [train_size,val_size,test_size])\n",
    "\n",
    "dataflow = dict({'train' : train_set, 'valid': val_set , 'test' : test_set})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 368\n",
      "valid 79\n",
      "test 80\n"
     ]
    }
   ],
   "source": [
    "for i in dataflow:\n",
    "    print(i, len(dataflow[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = HybridModel().to(device)\n",
    "model_without_qf = HybridModel_without_qf().to(device)\n",
    "n_epochs = 5\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-3, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    }
   ],
   "source": [
    "accu_list1 = []\n",
    "loss_list1 = []\n",
    "accu_list2 = []\n",
    "loss_list2 = []\n",
    "\n",
    "def train(dataflow, model, device, optimizer):\n",
    "    for feed_dict in dataflow['train']:\n",
    "        inputs = feed_dict['image'].to(device)\n",
    "        targets = feed_dict['can_type'].to(device)\n",
    "        #print(targets)\n",
    "        #print(targets.shape)\n",
    "        \n",
    "\n",
    "        outputs = model(inputs)\n",
    "        #print(outputs.shape)\n",
    "        #print(outputs)\n",
    "\n",
    "        #print(inputs)\n",
    "        #loss = F.nll_loss(outputs, targets)\n",
    "        loss = F.binary_cross_entropy(outputs[0][0].to(torch.float32), targets[0].to(torch.float32))\n",
    "        #loss = criterion(outputs[0], targets.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"loss: {loss.item()}\", end='\\r')\n",
    "\n",
    "\n",
    "def valid_test(dataflow, split, model, device, qiskit=False):\n",
    "    target_all = []\n",
    "    output_all = []\n",
    "    with torch.no_grad():\n",
    "        for feed_dict in dataflow[split]:\n",
    "            inputs = feed_dict['image'].to(device)\n",
    "            targets = feed_dict['can_type'].to(device)\n",
    "\n",
    "            outputs = model(inputs, use_qiskit=qiskit)\n",
    "\n",
    "            target_all.append(targets[0].to(torch.float32))\n",
    "            output_all.append(outputs[0][0].to(torch.float32))\n",
    "        target_all = torch.cat(target_all, dim=0)\n",
    "        output_all = torch.cat(output_all, dim=0)\n",
    "\n",
    "    _, indices = output_all.topk(1, dim=1)\n",
    "    masks = indices.eq(target_all.view(-1, 1).expand_as(indices))\n",
    "    size = target_all.shape[0]\n",
    "    corrects = masks.sum().item()\n",
    "    accuracy = corrects / size\n",
    "    loss = F.binary_cross_entropy(output_all, target_all).item()\n",
    "\n",
    "    print(f\"{split} set accuracy: {accuracy}\")\n",
    "    print(f\"{split} set loss: {loss}\")\n",
    "\n",
    "    return accuracy, loss\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # train\n",
    "    print(f\"Epoch {epoch}:\")\n",
    "    train(dataflow, model, device, optimizer)\n",
    "    print(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    # valid\n",
    "    accu, loss = valid_test(dataflow, 'test', model, device, )\n",
    "    accu_list1.append(accu)\n",
    "    loss_list1.append(loss)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
